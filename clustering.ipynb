{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zAWDvuE-npFF"
      },
      "outputs": [],
      "source": [
        "## Importing the necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from sklearn.metrics import classification_report\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "np.random.seed(7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iz8dmtBinpFO"
      },
      "outputs": [],
      "source": [
        "def load_embeddings():\n",
        "    \"\"\"\n",
        "    Loading the stress dataset in pandas dataframe\n",
        "    :return: scaled data\n",
        "    \"\"\"\n",
        "    # loading data\n",
        "    stress = pd.read_csv(\"data.csv\")\n",
        "    true_labels = stress['Label']\n",
        "    stress = stress.loc[:, stress.columns != 'Label']\n",
        "\n",
        "    # checking data shape\n",
        "    row, col = stress.shape\n",
        "    print(f'There are {row} rows and {col} columns')\n",
        "    print(stress.head(10))\n",
        "\n",
        "    # to work on copy of the data\n",
        "    stress_scaled = stress.copy()\n",
        "\n",
        "    # Scaling the data to keep the different attributes in same range.\n",
        "    stress_scaled[stress_scaled.columns] = StandardScaler().fit_transform(stress_scaled)\n",
        "    print(stress_scaled.describe())\n",
        "\n",
        "    return stress_scaled, true_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K2JM_pcMnpFO"
      },
      "outputs": [],
      "source": [
        "def pca_embeddings(df_scaled):\n",
        "    \"\"\"To reduce the dimensions of the stress dataset we use Principal Component Analysis (PCA).\n",
        "    Here we reduce it\n",
        "    :param df_scaled: scaled data\n",
        "    :return: pca result, pca for plotting graph\n",
        "    \"\"\"\n",
        "\n",
        "    pca_2 = PCA(n_components=2)\n",
        "    pca_2_result = pca_2.fit_transform(df_scaled)\n",
        "    print('Explained Variation per Principal Component: {}'.format(pca_2.explained_variance_ratio_))\n",
        "    print('Cumulative Variance Explained by 2 Principal Components: {:.2%}'.format(\n",
        "        np.sum(pca_2.explained_variance_ratio_)))\n",
        "    return pca_2_result, pca_2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I3237aw5npFO"
      },
      "outputs": [],
      "source": [
        "def kmean_hyper_param_tuning(data):\n",
        "    \"\"\"\n",
        "    s\n",
        "    :param data: dimensionality reduced data after applying PCA\n",
        "    :return: best number of clusters for the model (used for KMeans n_clusters)\n",
        "    \"\"\"\n",
        "    # candidate values for our number of cluster\n",
        "    parameters = [2, 3, 4] #2,3,4,5,10,15,20,25,30,35,40\n",
        "\n",
        "    # instantiating ParameterGrid, pass number of clusters as input\n",
        "    parameter_grid = ParameterGrid({'n_clusters': parameters})\n",
        "\n",
        "    best_score = -1\n",
        "    kmeans_model = KMeans()     # instantiating KMeans model\n",
        "    silhouette_scores = []\n",
        "\n",
        "    # evaluation based on silhouette_score\n",
        "    for p in parameter_grid:\n",
        "        kmeans_model.set_params(**p)    # set current hyper parameter\n",
        "        kmeans_model.fit(data)          #\n",
        "        ss = metrics.silhouette_score(data, kmeans_model.labels_)   # calculate silhouette_score\n",
        "        calinski_harabaz_score = metrics.calinski_harabasz_score(data, kmeans_model.labels_)\n",
        "        davies_bouldin_score = metrics.davies_bouldin_score(data, kmeans_model.labels_)\n",
        "        silhouette_scores += [ss]       # store all the scores\n",
        "\n",
        "        print('Parameter:', p, 'Score', ss)\n",
        "        print('Parameter:', p, 'Calinski Harabaz Score:', calinski_harabaz_score)\n",
        "        print('Parameter:', p, 'Davies Bouldin Score:', davies_bouldin_score)\n",
        "        print(\"  \")\n",
        "\n",
        "        # check p which has the best score\n",
        "        if ss > best_score:\n",
        "            best_score = ss\n",
        "            best_grid = p\n",
        "\n",
        "    # plotting silhouette score\n",
        "    plt.bar(range(len(silhouette_scores)), list(silhouette_scores), align='center', color='#722f59', width=0.5)\n",
        "    plt.xticks(range(len(silhouette_scores)), list(parameters))\n",
        "    plt.title('Silhouette Score', fontweight='bold')\n",
        "    plt.xlabel('Number of Clusters')\n",
        "    plt.show()\n",
        "\n",
        "    return best_grid['n_clusters']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wLQVlR9qnpFP"
      },
      "outputs": [],
      "source": [
        "def visualizing_results(pca_result, label, centroids_pca):\n",
        "    \"\"\" Visualizing the clusters\n",
        "    :param pca_result: PCA applied data\n",
        "    :param label: K Means labels\n",
        "    :param centroids_pca: PCA format K Means centroids\n",
        "    \"\"\"\n",
        "    # ------------------ Using Matplotlib for plotting-----------------------\n",
        "    x = pca_result[:, 0]\n",
        "    y = pca_result[:, 1]\n",
        "\n",
        "    plt.scatter(x, y, c=label, alpha=0.5, s= 200)  # plot different colors per cluster\n",
        "    plt.title('Psychological State Clusters')\n",
        "    plt.xlabel('PCA 1')\n",
        "    plt.ylabel('PCA 2')\n",
        "\n",
        "    plt.scatter(centroids_pca[:, 0], centroids_pca[:, 1], marker='X', s=200, linewidths=1.5,\n",
        "                color='red', edgecolors=\"black\", lw=1.5)\n",
        "\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxgty-pZnpFP"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    print(\"1. Loading Stress dataset\")\n",
        "    data_scaled, true_labels = load_embeddings()\n",
        "\n",
        "    print(\"2. Reducing via PCA\")\n",
        "    pca_result, pca_2 = pca_embeddings(data_scaled)\n",
        "\n",
        "    print(\"3. HyperTuning the Parameter for KMeans\")\n",
        "    optimum_num_clusters = kmean_hyper_param_tuning(data_scaled)\n",
        "    print(\"optimum num of clusters =\", optimum_num_clusters)\n",
        "\n",
        "    # fitting KMeans\n",
        "    kmeans = KMeans(n_clusters=optimum_num_clusters)\n",
        "    kmeans.fit(data_scaled)\n",
        "    centroids = kmeans.cluster_centers_\n",
        "    centroids_pca = pca_2.transform(centroids)\n",
        "\n",
        "    print(\"4. Visualizing the data\")\n",
        "    visualizing_results(pca_result, kmeans.labels_, centroids_pca)\n",
        "\n",
        "    print(\"Normalized Mutual Info Score(NMI): {}\".format(metrics.normalized_mutual_info_score(true_labels, kmeans.labels_)))\n",
        "    print(\"Adjusted Rand Score(ARI): {}\".format(metrics.adjusted_rand_score(true_labels, kmeans.labels_)))\n",
        "    print(\"Accuracy: {}\".format(metrics.accuracy_score(true_labels, kmeans.labels_)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lNotAPLRnpFP"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}